{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import vgg16\n",
    "import PIL\n",
    "from PIL import Image\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import tensorflow as tf\n",
    "from IPython.display import display"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "vgg16.data_dir = 'vgg16/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading VGG16 Model ...\n",
      "Data has apparently already been downloaded and unpacked.\n"
     ]
    }
   ],
   "source": [
    "vgg16.maybe_download()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def load_image(filename, max_size=None):\n",
    "    image = PIL.Image.open(filename)\n",
    "\n",
    "    if max_size is not None:\n",
    "        factor = max_size / np.max(image.size)\n",
    "        size = np.array(image.size) * factor\n",
    "        size = size.astype(int)\n",
    "\n",
    "        image = image.resize(size, PIL.Image.LANCZOS)\n",
    "    return np.float32(image)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def save_image(image, filename):\n",
    "    image = np.clip(image, 0.0, 255.0)\n",
    "    image = image.astype(np.uint8)\n",
    "    \n",
    "    with open(filename, 'wb') as file:\n",
    "        PIL.Image.fromarray(image).save(file, 'jpeg')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def plot_image_big(image):\n",
    "    image = np.clip(image, 0.0, 255.0)\n",
    "    image = image.astype(np.uint8)\n",
    "\n",
    "    display(PIL.Image.fromarray(image))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def plot_images(content_image, style_image, mixed_image):\n",
    "    fig, axes = plt.subplots(1, 3, figsize=(10, 10))\n",
    "    fig.subplots_adjust(hspace=0.1, wspace=0.1)\n",
    "    interpolation = 'sinc'\n",
    "    \n",
    "    ax = axes.flat[0]\n",
    "    ax.imshow(content_image / 255.0, interpolation=interpolation)\n",
    "    ax.set_xlabel(\"Content\")\n",
    "\n",
    "    ax = axes.flat[1]\n",
    "    ax.imshow(mixed_image / 255.0, interpolation=interpolation)\n",
    "    ax.set_xlabel(\"Mixed\")\n",
    "\n",
    "    ax = axes.flat[2]\n",
    "    ax.imshow(style_image / 255.0, interpolation=interpolation)\n",
    "    ax.set_xlabel(\"Style\")\n",
    "\n",
    "    for ax in axes.flat:\n",
    "        ax.set_xticks([])\n",
    "        ax.set_yticks([])\n",
    "    \n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def mean_squared_error(a, b):\n",
    "    return tf.reduce_mean(tf.square(a - b))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def create_content_loss(session, model, content_image, layer_ids):  \n",
    "    feed_dict = model.create_feed_dict(image=content_image)\n",
    "    layers = model.get_layer_tensors(layer_ids)\n",
    "\n",
    "    values = session.run(layers, feed_dict=feed_dict)\n",
    "    with model.graph.as_default():\n",
    "        layer_losses = []\n",
    "    \n",
    "        for value, layer in zip(values, layers):\n",
    "            value_const = tf.constant(value)\n",
    "            loss = mean_squared_error(layer, value_const)\n",
    "            layer_losses.append(loss)\n",
    "\n",
    "        total_loss = tf.reduce_mean(layer_losses)\n",
    "        \n",
    "    return total_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def gram_matrix(tensor):\n",
    "    shape = tensor.get_shape()\n",
    "    \n",
    "    num_channels = int(shape[3])\n",
    "    matrix = tf.reshape(tensor, shape=[-1, num_channels])\n",
    "    gram = tf.matmul(tf.transpose(matrix), matrix)\n",
    "\n",
    "    return gram                            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def create_style_loss(session, model, style_image, layer_ids):\n",
    "    \n",
    "    feed_dict = model.create_feed_dict(image=style_image)\n",
    "    layers = model.get_layer_tensors(layer_ids)\n",
    "\n",
    "    with model.graph.as_default():\n",
    "        gram_layers = [gram_matrix(layer) for layer in layers]\n",
    "\n",
    "        values = session.run(gram_layers, feed_dict=feed_dict)\n",
    "        layer_losses = []\n",
    "        for value, gram_layer in zip(values, gram_layers):\n",
    "            value_const = tf.constant(value)\n",
    "            loss = mean_squared_error(gram_layer, value_const)\n",
    "            layer_losses.append(loss)\n",
    "        total_loss = tf.reduce_mean(layer_losses)\n",
    "        \n",
    "    return total_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def create_denoise_loss(model):\n",
    "    loss = tf.reduce_sum(tf.abs(model.input[:,1:,:,:] - model.input[:,:-1,:,:])) + \\\n",
    "           tf.reduce_sum(tf.abs(model.input[:,:,1:,:] - model.input[:,:,:-1,:]))\n",
    "\n",
    "    return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def style_transfer(content_image, style_image,\n",
    "                   content_layer_ids, style_layer_ids,\n",
    "                   weight_content=1.5, weight_style=10.0,\n",
    "                   weight_denoise=0.3,\n",
    "                   num_iterations=120, step_size=10.0):\n",
    "    \n",
    "    model = vgg16.VGG16()\n",
    "    session = tf.InteractiveSession(graph=model.graph)\n",
    "\n",
    "    print(\"Content layers:\")\n",
    "    print(model.get_layer_names(content_layer_ids))\n",
    "    print()\n",
    "\n",
    "    print(\"Style layers:\")\n",
    "    print(model.get_layer_names(style_layer_ids))\n",
    "    print()\n",
    "\n",
    "    loss_content = create_content_loss(session=session,\n",
    "                                       model=model,\n",
    "                                       content_image=content_image,\n",
    "                                       layer_ids=content_layer_ids)\n",
    "\n",
    "    loss_style = create_style_loss(session=session,\n",
    "                                   model=model,\n",
    "                                   style_image=style_image,\n",
    "                                   layer_ids=style_layer_ids)    \n",
    "\n",
    "    loss_denoise = create_denoise_loss(model)\n",
    "\n",
    "    adj_content = tf.Variable(1e-10, name='adj_content')\n",
    "    adj_style = tf.Variable(1e-10, name='adj_style')\n",
    "    adj_denoise = tf.Variable(1e-10, name='adj_denoise')\n",
    "\n",
    "    session.run([adj_content.initializer,\n",
    "                 adj_style.initializer,\n",
    "                 adj_denoise.initializer])\n",
    "\n",
    "    update_adj_content = adj_content.assign(1.0 / (loss_content + 1e-10))\n",
    "    update_adj_style = adj_style.assign(1.0 / (loss_style + 1e-10))\n",
    "    update_adj_denoise = adj_denoise.assign(1.0 / (loss_denoise + 1e-10))\n",
    "\n",
    "    loss_combined = weight_content * adj_content * loss_content + \\\n",
    "                    weight_style * adj_style * loss_style + \\\n",
    "                    weight_denoise * adj_denoise * loss_denoise\n",
    "\n",
    "    gradient = tf.gradients(loss_combined, model.input)\n",
    "\n",
    "    run_list = [gradient, update_adj_content, update_adj_style, \\\n",
    "                update_adj_denoise]\n",
    "\n",
    "    mixed_image = np.random.rand(*content_image.shape) + 128\n",
    "\n",
    "    for i in range(num_iterations):\n",
    "        feed_dict = model.create_feed_dict(image=mixed_image)\n",
    "\n",
    "        grad, adj_content_val, adj_style_val, adj_denoise_val \\\n",
    "        = session.run(run_list, feed_dict=feed_dict)\n",
    "\n",
    "        grad = np.squeeze(grad)\n",
    "\n",
    "        step_size_scaled = step_size / (np.std(grad) + 1e-8)\n",
    "        mixed_image -= grad * step_size_scaled\n",
    "\n",
    "        mixed_image = np.clip(mixed_image, 0.0, 255.0)\n",
    "        print(\". \", end=\"\")\n",
    "        if (i % 10 == 0) or (i == num_iterations - 1):\n",
    "            print()\n",
    "            print(\"Iteration:\", i)\n",
    "\n",
    "            msg = \"Weight Adj. for Content: {0:.2e}, Style: {1:.2e}, Denoise: {2:.2e}\"\n",
    "            print(msg.format(adj_content_val, adj_style_val, adj_denoise_val))\n",
    "            plot_images(content_image=content_image,\n",
    "                        style_image=style_image,\n",
    "                        mixed_image=mixed_image)\n",
    "            \n",
    "    print()\n",
    "    print(\"Final image:\")\n",
    "    plot_image_big(mixed_image)\n",
    "    session.close()\n",
    "    return mixed_image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "content_filename = 'images/city2.jpg'\n",
    "content_image = load_image(content_filename, max_size=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "style_filename = 'images/style18.jpg'\n",
    "style_image = load_image(style_filename, max_size=300)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "content_layer_ids = [4]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "style_layer_ids = list(range(13))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "img = style_transfer(content_image=content_image,\n",
    "                     style_image=style_image,\n",
    "                     content_layer_ids=content_layer_ids,\n",
    "                     style_layer_ids=style_layer_ids,\n",
    "                     weight_content=4.0,\n",
    "                     weight_style=10.0,\n",
    "                     weight_denoise=0.5,\n",
    "                     num_iterations=3200,\n",
    "                     step_size=1.5)"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [conda root]",
   "language": "python",
   "name": "conda-root-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
